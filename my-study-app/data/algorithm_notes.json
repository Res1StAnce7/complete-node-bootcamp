{
  "Time Complexity": [
    "$\\textbf{Upper and Lower Bounds in Time Complexity}$\n\n$\\textbf{Upper Bound (Big-O):}$\n- Denoted as $O(f(n))$\n- Represents the worst-case growth rate\n- Function $g(n)$ is $O(f(n))$ if there exist positive constants $c$ and $n_0$ such that:\n  $0 \\leq g(n) \\leq c \\cdot f(n)$ for all $n \\geq n_0$\n\n$\\textbf{Lower Bound (Omega):}$\n- Denoted as $\\Omega(f(n))$\n- Represents the best-case growth rate\n- Function $g(n)$ is $\\Omega(f(n))$ if there exist positive constants $c$ and $n_0$ such that:\n  $0 \\leq c \\cdot f(n) \\leq g(n)$ for all $n \\geq n_0$\n\n$\\textbf{Tight Bound (Theta):}$\n- Denoted as $\\Theta(f(n))$\n- Function is both $O(f(n))$ and $\\Omega(f(n))$\n- We say $g(n)$ is $\\Theta(f(n))$ if $g(n)$ is bounded both above and below by $f(n)$\n- Mathematically: $g(n)$ is $\\Theta(f(n))$ if there exist positive constants $c_1$, $c_2$, and $n_0$ such that:\n  $c_1 \\cdot f(n) \\leq g(n) \\leq c_2 \\cdot f(n)$ for all $n \\geq n_0$\n\nExample:\n- Consider $g(n) = 3n^2 + 2n + 1$\n- Upper bound: $O(n^2)$ because eventually $3n^2 + 2n + 1 \\leq cn^2$ for some constant $c$\n- Lower bound: $\\Omega(n^2)$ because eventually $3n^2 + 2n + 1 \\geq cn^2$ for some constant $c$\n- Therefore, $g(n) = \\Theta(n^2)$ as it's both $O(n^2)$ and $\\Omega(n^2)$",
    "Average Case and Amortized Analysis\n\nAverage Case Analysis:\n- Examines the expected behavior under random input\n- Based on probability distribution of inputs\n- Example: QuickSort\n  * Worst case: $O(n^2)$ when pivot is always smallest/largest\n  * Average case: $O(n \\log n)$ when considering random input distributions\n  * Probability analysis shows bad pivot selections are rare\n\nAmortized Analysis:\n- Studies the averaged time per operation over a sequence of operations\n- Important when single operations can have varying costs\n- Three main methods:\n  1. Aggregate Method\n     - Total cost of $\\frac{n \\text{ operations}}{n} = \\frac{T(n)}{n}$\n     - each operation will get the same amortized cost, even if there are several \n       types of operations in the sequence.\n\n     Example:\n     Unbounded Array Growth Analysis\n\n     | Insert | Old size | New size | Copy Cost |\n     |--------|----------|----------|-----------|\n     |   1    |    1     |    -     |     -     |\n     |   2    |    1     |    2     |     1     |\n     |   3    |    2     |    4     |     2     |\n     |   4    |    4     |    -     |     -     |\n     |   5    |    4     |    8     |     4     |\n     |   6    |    8     |    -     |     -     |\n     |   7    |    8     |    -     |     -     |\n     |   8    |    8     |    -     |     -     |\n     |   9    |    8     |   16     |     8     |\n\n     #inserts = 9, #copy = 1 + 2 + 4 + 8 = 15\n     #inserts = $2^n + 1$, #copy = $1 + 2 + 4 + \\text{...} + 2^n = 2^{n+1} - 1$\n     $T(n) = 2^n + 1 + 2^{n+1} - 1 = 3 * 2^n$\n     AC = $\\frac{\\text{total cost}}{\\text{# inserts}} = \\frac{3 * 2^n}{2^n + 1} =\n     \\lim_{n \\to \\infty} = 3$\n\n  \n  2. Accounting (Banker's) Method\n     - Assigns different charges to different operations\n     - Computes the individual cost of each operation\n     - Saves \"credits\" from cheap operations for expensive ones\n     - Only practical if we guess that the amortized cost is constant\n     - Example: Dynamic Array Resizing\n       * Most insertions cost $1$ (cheap)\n       * Some insertions trigger resize, cost $n$ (expensive)\n       * Each cheap insertion saves credit for eventual resize\n  \n  3. Potential Method\n     - Uses a potential function $\\Phi$ to track \"stored energy\"\n     - Amortized cost = actual cost + $\\Phi(\\text{after}) - \\Phi(\\text{before})$\n\nClassic Example - Dynamic Array:\n- Doubling array size when full\n- Single insert might cost $O(n)$ for resize\n- But amortized analysis shows $O(1)$ per insertion because:\n  * $n$ insertions cost $O(n)$ total\n  * Spreading cost: $\\frac{O(n)}{n} = O(1)$ per operation\n  * Each element is copied at most $\\log n$ times total",
    "Polynomial functions grow faster than logarithmic functions. Specifically:\n\n$\\log n = O(n)$\n$\\log^2 n = O(n)$\n$\\log^{1000} n = O(n)$\n\nExponential functions grow faster than polynomial ones. For example:\n\n$n = O(2^n)$\n$n^{10} = O(2^n)$\n$n^{1000} = O(2^n)$\n\n$n^{1/\\log n} = n$\n$2^{\\log n} = n$",
    "Discussion Problem 2\nSuppose that $f(n)$ and $g(n)$ are two positive non-decreasing functions such that $f(n) = O(g(n))$. Is it true that $2^{f(n)} = O(2^{g(n)})$? \n\n**Counterexample**\n\n**Choose the following functions:**\n\n- $g(n) = n$\n- $f(n) = 2n$\n\n**Verification:**\n\n1. **Check $f(n) = O(g(n))$:**\n- By definition, $f(n) = O(g(n))$ means there exist constants $C > 0$ and $n_0$ such that for all $n \\geq n_0$,\n\n$f(n) \\leq C \\cdot g(n)$\n\n- Here, $f(n) = 2n$ and $g(n) = n$. Choose $C = 2$ and $n_0 = 1$. Then for all $n \\geq 1$,\n\n$2n \\leq 2 \\cdot n$\n\nwhich holds true. Thus, $f(n) = O(g(n))$.\n\n2. **Examine $2^{f(n)}$ vs. $2^{g(n)}$:**\n- Compute $2^{f(n)}$ and $2^{g(n)}$:\n\n$2^{f(n)} = 2^{2n} = (2^n)^2 = 4^n$\n\n$2^{g(n)} = 2^n$\n\n- To determine if $2^{f(n)} = O(2^{g(n)})$, we need to check if there exist constants $C' > 0$ and $n_0'$ such that for all $n \\geq n_0'$,\n\n$4^n \\leq C' \\cdot 2^n$\n\n- Simplifying the inequality:\n\n$4^n = (2^2)^n = 2^{2n} = (2^n)^2$\n\n$\\Rightarrow 2^{2n} \\leq C' \\cdot 2^n$\n\n$\\Rightarrow 2^n \\leq C'$\n\n- The inequality $2^n \\leq C'$ cannot hold for all $n$ as $n$ grows, because $2^n$ increases exponentially without bound. Therefore, no such constant $C'$ exists."
  ],
  "Graph": [
    "Let $G$ be an undirected graph with $V$ vertices and $E$ edges. The following statements are equivalent:\n\n1. $G$ is a tree (a connected graph with no cycles).\n\n2. Every two vertices of $G$ are connected by a unique path.\n\n3. $G$ is connected and $V = E + 1$.\n\n4. $G$ is acyclic and $V = E + 1$.\n\n5. $G$ is acyclic and if any two non-adjacent vertices are joined by an edge, the resulting graph has exactly one cycle.\n",
    "Adjacency List Representation is used for representation of the sparse $(E = O(V))$ graphs.\n\nAdjacency Matrix Representation is used for representation of the dense $(E = \\Omega(V^2))$ graphs.\n\n**Theorem**: Prove that in an undirected simple graph $G = (V, E)$, there are at most $V(V-1)/2$ edges.\n$n + (n - 1) + (n - 2) + (n - 3) + ... + 1 = o(n^2)$ \nIn short, using the asymptotic notation, $E = O(V^2)$.",
    "**Depth-First-Search (DFS)**: It starts at a selected node and explores as far as possible along each branch before backtracking. DFS uses a stack for backtracking.\n\n**Breadth-First-Search (BFS)**: It starts at a selected node and explores all nodes at the present depth prior to moving on to the nodes at the next depth level. BFS uses a FIFO queue for bookkeeping.\n\nRuntime complexity: $O(V + E)$\n\n**Property 1**: They visit all the vertices in the _connected_ component.\n\n**Property 2**: The result of traversal is a _spanning tree_ of the connected\ncomponent.",
    "A graph is planar if it can be drawn in the plane without crossing edges.\n\nIf $G$ is a connected planar graph with $V$vertices, $E$ edges and $F$ faces, then $V – E + F = 2$. \n\nAny simple planar graph can be colored with less than or equal to 4 colors."
  ],
  "Recurrence Relations": [],
  "Master Theorem": [],
  "Amortized Analysis": [],
  "Sorting Algorithms": [],
  "Space Complexity": [],
  "S": [],
  "Recurrence ": [],
  "Heaps": [
    "$\\textbf{Heap Data Structures Runtime Complexity Comparison}$\n\n| Operation          | Binary Heap | Binomial Heap | Fibonacci Heap |\n|-------------------|-------------|---------------|----------------|\n| find-min          | $O(1)$      | $O(\\log n)$   | $O(1)$         |\n| insert            | $O(\\log n)$ | $O(\\log n)$   | $O(1)$         |\n| extract-min       | $O(\\log n)$ | $O(\\log n)$   | $O(\\log n)$    |\n| decrease-key      | $O(\\log n)$ | $O(\\log n)$   | $O(1)^*$       |\n| delete            | $O(\\log n)$ | $O(\\log n)$   | $O(\\log n)$    |\n| merge/union       | $O(n)$      | $O(\\log n)$   | $O(1)$         |\n| build heap        | $O(n)$      | $O(n)$        | $O(n)$         |\n\n$\\textbf{Notes}$:\n* All complexities shown are amortized except where noted\n* $^*$ Fibonacci heap decrease-key is amortized $O(1)$\n* Binary heap has simplest implementation\n* Binomial heap better for frequent merges\n* Fibonacci heap has best theoretical bounds but complex implementation\n\n$\\textbf{Space Complexity}$:\n* Binary Heap: $O(n)$\n* Binomial Heap: $O(n)$\n* Fibonacci Heap: $O(n)$",
    "$\\textbf{Binary Heap Runtime Complexities:}$\n\n$\\textbf{Basic Operations:}$\n- get_min/get_max: $O(1)$ \n- insert: $O(\\log n)$\n- extract_min/extract_max: $O(\\log n)$\n- delete: $O(\\log n)$\n- decrease_key: $O(\\log n)$\n- increase_key: $O(\\log n)$\n\n$\\textbf{Helper Operations:}$\n- heapify_up (bubble_up): $O(\\log n)$\n- heapify_down (bubble_down): $O(\\log n)$\n\n$\\textbf{Special Operations:}$\n- build_heap: $O(n)$ [Not $O(n \\log n)$ as might be expected]\n- merge heaps: $O(n)$ [For naive implementation copying all elements]\n\n$\\textbf{Space Complexity:}$\n- Array representation: $O(n)$",
    "$\\textbf{Binomial Heap Structure}$\n- Collection of binomial trees\n- Each binomial tree $B_k$ has:\n * Root with degree k\n * $2^k$ total nodes\n * Height k\n * Level i has $\\binom{k}{i}$ nodes\n * Order of tree = number of nodes = $2^k$\n\n$\\textbf{Basic Properties}$\n- A binomial heap of n nodes has at most $\\lfloor \\log n \\rfloor + 1$ trees\n- Binary representation of n corresponds to which $B_k$ trees are present\n- Trees are ordered by root values (min-heap or max-heap property)\n- Each $B_k$ appears 0 or 1 times\n\n$\\textbf{Operations Runtime}$\n- find-min: $O(\\log n)$ [scan all roots]\n- insert: $O(\\log n)$ [like binary addition]\n- merge: $O(\\log n)$ [like binary addition]\n- extract-min: $O(\\log n)$ [remove min, merge children]\n- decrease-key: $O(\\log n)$ [bubble up]\n- delete: $O(\\log n)$ [decrease to -∞, extract-min]\n\n$\\textbf{Structure Operations}$\n- Link two $B_{k-1}$ trees: $O(1)$\n * Makes a $B_k$ tree\n * Compare roots, larger becomes child\n- Union/Merge: $O(\\log n)$\n * Like binary number addition\n * Merge same-degree trees\n\n$\\textbf{Example}$\n8 nodes = $1000_2$ binary = one $B_3$ tree\n10 nodes = $1010_2$ binary = one $B_3$ and one $B_1$ tree\n\n$\\textbf{Advantages}$\n- Union/Merge in $O(\\log n)$ [better than binary heap's $O(n)$]\n- Supports decrease-key efficiently\n- Good amortized performance\n- Simple implementation compared to Fibonacci heaps\n\n$\\textbf{Applications}$\n- Priority queues\n- Dijkstra's algorithm\n- Prim's algorithm\n- Other graph algorithms needing efficient decrease-key",
    "$\\textbf{Given a sequence of numbers: 3, 5, 2, 8, 1, 5, 2, 7.}$\n$\\textbf{Draw a binomial heap by inserting the above numbers reading them from left to right.}$\n\nInitial state: Empty heap\n\nInsert 3: B₀\n3\n\nInsert 5: Link B₀ + B₀ → B₁\n3\n└─ 5\n\nInsert 2: B₀ + B₁\n2   3\n    └─ 5\n\nInsert 8: Link B₀ + B₀ → B₁, then have B₁ + B₁ → B₂\n2\n└─ 3\n   ├─ 5\n   └─ 8\n\nInsert 1: B₀ + B₂\n1   2\n    └─ 3\n       ├─ 5\n       └─ 8\n\nInsert 5: Link B₀ + B₀ → B₁, then have B₁ + B₂ → B₃\n1\n└─ 2\n   ├─ 3\n   │  ├─ 5\n   │  └─ 8\n   └─ 5\n\nInsert 2: B₀ + B₃\n2   1\n    └─ 2\n       ├─ 3\n       │  ├─ 5\n       │  └─ 8\n       └─ 5\n\nInsert 7: Link B₀ + B₀ → B₁, then B₁ + B₃ → B₄\n1\n└─ 2\n   ├─ 2\n   │  └─ 7\n   └─ 3\n      ├─ 5\n      └─ 8\n      └─ 5",
    "$\\textbf{How many binomial trees does a binomial heap with 25 elements contain?}$\n$\\textbf{What are the ranks of those trees?}$\n\nConvert 25 to binary:\n* 25 (decimal) = 11001 (binary)\n* 25 = $2^4 + 2^3 + 2^0$\n* 25 = 16 + 8 + 1\n\n$\\textbf{Analysis}$:\n* Each 1 in binary represents a binomial tree\n* Position (from right) indicates rank\n* For 11001:\n - 1 in $2^0$ position → $B_0$ tree\n - 0 in $2^1$ position → no $B_1$ tree\n - 0 in $2^2$ position → no $B_2$ tree\n - 1 in $2^3$ position → $B_3$ tree\n - 1 in $2^4$ position → $B_4$ tree\n\n$\\textbf{Answer}$:\n* Number of trees: 3 binomial trees\n* Ranks: $B_0$, $B_3$, and $B_4$\n\n$\\textbf{Verification}$:\n$2^0 + 2^3 + 2^4 = 1 + 8 + 16 = 25$ elements total"
  ]
}